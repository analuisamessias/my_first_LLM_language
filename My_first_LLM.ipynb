{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1Z73MqvOLz7xhhFCW3Ffb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/analuisamessias/my_first_LLM_language/blob/main/My_first_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bem vindo(a) ao primeiro teste do meu chatbot**\n",
        "*Por Ana Luisa Messias Ferreira Mendes*\n",
        "\n",
        "```\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ :) WELCOME :) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ :) WELCOME :) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "```\n",
        "Este projeto foi criado durante a semana da Imers√£o AI com [alura](https://www.alura.com.br/) & [Google](https://www.google.com.br/).\n",
        "\n",
        "Para desenvolvimento desse projeto, criamos um chatbot usando a [GEMINI](https://gemini.google.com/app) API do google. O GEMINI ‚ú¥ √© a AI desenvolvida pela Google. Uma ferramenta simples de usar, que pode entregar muitos resultados para voc√™!\n",
        "\n",
        "Voc√™ tamb√©m pode desenvolver **seu pr√≥prio chatbot**! Saiba mais em: [Google AI for Developers](https://ai.google.dev/gemini-api/docs/quickstart)\n"
      ],
      "metadata": {
        "id": "SHCZRL3KBKLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando o SDK do Google\n",
        "\n",
        "Instalando a depend√™ncia usando o pip"
      ],
      "metadata": {
        "id": "aKQ73s5lFkyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFo3x67K_gif"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up your API key\n",
        "\n",
        "Para usar o GEMINI API, voc√™ precisa de uma API key üîë. Se voc√™ n√£o tem uma API key, voc√™ pode criar uma no Google AI Studio üíª, acesse o link abaixo!\n",
        "\n",
        "[Crie sua API Key](https://aistudio.google.com/app/apikey)"
      ],
      "metadata": {
        "id": "-oG2JI9ZF8vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a biblioteca google generative ai\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY='INSIRA SUA API KEY'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "WInmhYSiGcMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lista dos Modelos dispon√≠veis\n",
        "\n",
        "Abaixo, voc√™ pode visualizar quais os modelos do GEMINI ‚ú¥ est√£o dispon√≠veis para uso."
      ],
      "metadata": {
        "id": "XmwEEPTWGuVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Z8G96jatHNku",
        "outputId": "dfbede30-d693-4966-d64c-8a41de02c9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo est√£o algumas configura√ß√µes do modelo.\n",
        "Para entender melhor: Temperatura, top_p e top_k dizema respeito da criatividade do chat. Ou seja, quanto maior (= 1) o chat ir√° usar mais palavras diferenciadas, e dar respostas mais criativas para uma mesma pergunta.\n",
        "\n",
        "As configura√ß√µes de seguran√ßa dizem a respeito sobre as respostas com conte√∫do de √≥dio, abusivo, conte√∫do sexual, ou perigoso.\n",
        "```\n",
        "BLOCK_MOST = Bloqueie tudo\n",
        "BLOCK_NONE = Bloqueio 0\n",
        "```"
      ],
      "metadata": {
        "id": "sh6nPz48KhCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    'candidate_count': 1,\n",
        "    'temperature': 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "La4zmQV3J0aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = {\n",
        "    'HARASSMENT': 'BLOCK_NONE',\n",
        "    'HATE': 'BLOCK_NONE',\n",
        "    'SEXUAL': 'BLOCK_NONE',\n",
        "    'DANGEROUS': 'BLOCK_NONE',\n",
        "}"
      ],
      "metadata": {
        "id": "TBvQzBroKZVi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializando o modelo. Para isso, escolhemos o modelo mais est√°vel ```Gemini 1.0 Pro``` e as configura√ß√µes que colocamos em cima."
      ],
      "metadata": {
        "id": "fgLY-Jz8MJE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings= safety_settings)"
      ],
      "metadata": {
        "id": "rCimW2irMLfG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos fazer um teste de resposta!\n",
        "\n",
        "Vou pedir para me dar uma receita de pudim de leite condensado! ü§§"
      ],
      "metadata": {
        "id": "ijwsmx9aNKha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Me d√™ uma receita de pudim de leite condensado\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "q8J0XxmYNNnD",
        "outputId": "3136e9d8-e10a-4e10-f9ee-e89b93054283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Ingredientes:**\n",
            "\n",
            "* 1 lata de leite condensado (395g)\n",
            "* 2 x√≠caras (ch√°) de leite integral\n",
            "* 3 ovos\n",
            "* 1 colher (ch√°) de ess√™ncia de baunilha\n",
            "\n",
            "**Para a calda:**\n",
            "\n",
            "* 1 x√≠cara (ch√°) de a√ß√∫car\n",
            "* 1/2 x√≠cara (ch√°) de √°gua\n",
            "\n",
            "**Modo de Preparo:**\n",
            "\n",
            "**Calda:**\n",
            "\n",
            "1. Em uma panela, misture o a√ß√∫car e a √°gua.\n",
            "2. Leve ao fogo m√©dio e deixe ferver at√© que o a√ß√∫car derreta e forme uma calda dourada.\n",
            "3. Despeje a calda em uma forma de pudim (20 cm de di√¢metro) e reserve.\n",
            "\n",
            "**Pudim:**\n",
            "\n",
            "1. No liquidificador, bata o leite condensado, o leite, os ovos e a ess√™ncia de baunilha at√© obter uma mistura homog√™nea.\n",
            "2. Despeje a mistura na forma caramelizada.\n",
            "3. Cubra a forma com papel alum√≠nio e leve ao forno preaquecido a 180¬∞C em banho-maria (coloque a forma dentro de uma assadeira com √°gua quente).\n",
            "4. Asse por cerca de 1 hora e 15 minutos, ou at√© que o pudim esteja firme e o palito inserido no centro saia limpo.\n",
            "5. Retire do forno e deixe esfriar completamente.\n",
            "6. Leve √† geladeira por pelo menos 4 horas antes de desenformar.\n",
            "\n",
            "**Para Desenformar:**\n",
            "\n",
            "1. Passe uma faca ao redor da borda do pudim.\n",
            "2. Vire a forma sobre um prato e agite levemente para desenformar.\n",
            "3. Sirva gelado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Criando o ChatBot**\n",
        "\n",
        "Agora, abaixo voc√™ consegue ver a cria√ß√£o do Chat_bot para conversa√ß√£o entre o usu√°rio e a AI."
      ],
      "metadata": {
        "id": "6xgtzSXcOyGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "_xcqNLLSO-y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quando voc√™ executar essa c√©lula, o programa abrir√° a caixa de di√°logo para\n",
        "# voc√™ inserir sua pergunta ou comando\n",
        "\n",
        "# Para finalizar a conversa, basta digitar 'sair'\n",
        "prompt = input(\"Digite sua pergunta ou comando: \")\n",
        "\n",
        "while prompt != \"sair\":\n",
        "  response = chat.send_message(prompt)\n",
        "  print(\"Resposta: \", response.text, \"\\n\")\n",
        "  prompt = input(\"Digite sua pergunta ou comando: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "WY3kW4ppP9fo",
        "outputId": "2cba25bf-282b-490b-8268-9e6c8044bb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digite sua pergunta ou comando: Qual a capital do Brasil?\n",
            "Resposta:  Bras√≠lia \n",
            "\n",
            "Digite sua pergunta ou comando: Quantos habitantes o Brasil possui?\n",
            "Resposta:  215 milh√µes (estimativa de 2023) \n",
            "\n",
            "Digite sua pergunta ou comando: sair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Melhorando a visualiza√ß√£o\n",
        "#C√≥digo dispon√≠vel em https://ai.google.dev/tutorials/python_quickstart#import_packages\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('‚Ä¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "#Imprimindo o hist√≥rico\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "  print('-------------------------------------------')"
      ],
      "metadata": {
        "id": "b4yyHbmeW0QD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}